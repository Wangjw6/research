{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Recurrent Neural Network for Multivariate Time Series with Missing Values](https://www.nature.com/articles/s41598-018-24271-9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. low-level GRU implementation [reference](https://gist.github.com/kmjjacobs/eab1e840aecf0ac232cc8370a9be9093)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def weight_variable(shape,train=True):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial,trainable=train)\n",
    "def bias_variable(shape,train=True):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial,trainable=train)\n",
    "\n",
    "class GRU():\n",
    "    \n",
    "    def __init__(self, input_dimensions,hidden_size,time_steps, dtype=tf.float32):\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
    "        self.Wr = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wr')\n",
    "        self.Wz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wz')\n",
    "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
    "        \n",
    "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
    "        self.Ur = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ur')\n",
    "        self.Uz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uz')\n",
    "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
    "        \n",
    "        # Biases for hidden vectors of shape (hidden_size,)\n",
    "        self.br = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='br')\n",
    "        self.bz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bz')\n",
    "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
    "        \n",
    "        # Define the input layer placeholder\n",
    "        self.input_holder = tf.placeholder(dtype=dtype, shape=(None, time_steps, input_dimensions), name='input')\n",
    "        \n",
    "        \n",
    "        # Put the time-dimension upfront for the scan operator\n",
    "        self.x_t = tf.transpose(self.input_holder, [1, 0, 2], name='x_t')\n",
    "        \n",
    "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
    "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=dtype, shape=(input_dimensions, hidden_size)), name='h_0')\n",
    "        \n",
    "        # Perform the scan operator\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
    "        \n",
    "        # Transpose the result back\n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "\n",
    "    def forward_pass(self, h_tm1, x_t):\n",
    "        \"\"\"Perform a forward pass.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h_tm1: np.matrix\n",
    "            The hidden state at the previous timestep (h_{t-1}).\n",
    "        x_t: np.matrix\n",
    "            The input vector.\n",
    "        \"\"\"\n",
    "        # Definitions of update gate\n",
    "        z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
    "        # Definition of reset gate\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
    "        \n",
    "        # Definition of candidate actvation\n",
    "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
    "        \n",
    "        # Compute the actual next hidden state\n",
    "        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
    "        \n",
    "        return h_t\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GRU implementation test ([data produce](https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf20_RNN2.2/full_code.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.511239\n",
      "loss: 0.501083\n",
      "loss: 0.459234\n",
      "loss: 0.440753\n",
      "loss: 0.418487\n",
      "loss: 0.360051\n",
      "loss: 0.372068\n",
      "loss: 0.387811\n",
      "loss: 0.315595\n",
      "loss: 0.298827\n",
      "loss: 0.310882\n",
      "loss: 0.303998\n",
      "loss: 0.262455\n",
      "loss: 0.274973\n",
      "loss: 0.296207\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt5\n",
    "\n",
    "BATCH_START = 0\n",
    "TIME_STEPS = 20\n",
    "BATCH_SIZE = 50\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "HIDDEN_SIZE = 12\n",
    "LR = 0.006\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def get_batch():\n",
    "    global BATCH_START, TIME_STEPS\n",
    "    # xs shape (50batch, 20steps)\n",
    "    xs = np.arange(BATCH_START, BATCH_START+TIME_STEPS*BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (10*np.pi)\n",
    "    seq = np.sin(xs)\n",
    "    res = np.cos(xs)\n",
    "    BATCH_START += TIME_STEPS\n",
    "    # plt.plot(xs[0, :], res[0, :], 'r', xs[0, :], seq[0, :], 'b--')\n",
    "    # plt.show()\n",
    "    # returned seq, res and xs: shape (batch, step, input)\n",
    "    return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]\n",
    "\n",
    "model = GRU(input_dimensions=INPUT_SIZE , hidden_size=HIDDEN_SIZE, time_steps=TIME_STEPS )\n",
    "output_holder = tf.placeholder(tf.float32,[None,TIME_STEPS,OUTPUT_SIZE])\n",
    "w_out = weight_variable([HIDDEN_SIZE,OUTPUT_SIZE])\n",
    "b_out = bias_variable([OUTPUT_SIZE])\n",
    "\n",
    "h = tf.reshape(model.h_t,(-1,HIDDEN_SIZE))\n",
    "predict = tf.matmul(h,w_out)+b_out\n",
    "predict = tf.reshape(predict,(-1,TIME_STEPS,OUTPUT_SIZE))\n",
    "loss = tf.reduce_mean((predict-output_holder)**2)\n",
    "train_Op = tf.train.AdamOptimizer(learning_rate=LR).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "plt.show()\n",
    "for i in range(300):\n",
    "        seq, res, xs = get_batch()\n",
    "        \n",
    "        feed_dict = {\n",
    "                    model.input_holder: seq,\n",
    "                    output_holder: res\n",
    "            }\n",
    "\n",
    "\n",
    "        _, loss_, pred = sess.run(\n",
    "            [train_Op, loss,predict],\n",
    "            feed_dict=feed_dict)\n",
    "        if i%20==0:\n",
    "            print('loss: %g'%(loss_))\n",
    "        plt.plot(xs[0, :], res[0].flatten(), 'r', xs[0, :], pred.flatten()[:TIME_STEPS], 'b--')\n",
    "        plt.ylim((-1.2, 1.2))\n",
    "        plt.draw()\n",
    "        plt.pause(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Literature model implementation with little modification for fitting transportation problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 61, 144)\n",
      "(214, 8784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import tensorly as ts\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.chdir(\"G:/jupyter/Guangzhou-data-set\")\n",
    "tensor = scipy.io.loadmat('tensor.mat')\n",
    "tensor = tensor['tensor']\n",
    "tensor = np.array(tensor)\n",
    "print(tensor.shape)\n",
    "tensor = tensor.reshape(214,-1)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_D():\n",
    "    def __init__(self,network_size, input_len, hidden_size,output_len):\n",
    "        \n",
    "        self.input_holder = tf.placeholder(tf.float32,[None,input_len*network_size])\n",
    "        self.mask_holder = tf.placeholder(tf.float32,[None,input_len*network_size])\n",
    "        self.base_observation_holder = tf.placeholder(tf.float32,[None,input_len*network_size]) # last observation in literature modified\n",
    "        self.decay_affector = tf.placeholder(tf.float32,[None,input_len*network_size])\n",
    "        self.emprical_holder = tf.placeholder(tf.float32,[None,input_len*network_size]) # modified\n",
    "        self.output_holder = tf.placeholder(tf.float32,[None,output_len*network_size])\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "                \n",
    "        self.input_dimensions = network_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        h_size = 32\n",
    "        \n",
    "        # impute input\n",
    "        mask = tf.eye(self.input_len*self.network_size)\n",
    "        ## diagonal to make variable indepent from each other\n",
    "        w_d1 = tf.multiply(mask,weight_variable([self.input_len*self.network_size,self.input_len*self.network_size]))\n",
    "        b_d1 = bias_variable([self.input_len*self.network_size])\n",
    "        self.input_decay = tf.exp(-tf.maximize(tf.matmul(self.decay_affector,wd1)+bd1,0))\n",
    "        \n",
    "        self.input_holder = tf.multiply(self.mask_holder,self.input_holder)\n",
    "        + tf.multiply(1-self.mask_holder,(tf.multiply(self.input_decay,self.base_observation_holder) + tf.multiply(1-self.input_decay, self.emprical_holder)))\n",
    "        \n",
    "        # impute feature\n",
    "        self.w_d2 = weight_variable([ self.network_size,self.hidden_size])\n",
    "        self.b_d2 = bias_variable([ self.hidden_size])\n",
    "      \n",
    "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
    "        self.Wr = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wr')\n",
    "        self.Wz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wz')\n",
    "        self.Wh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
    "        \n",
    "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
    "        self.Ur = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ur')\n",
    "        self.Uz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uz')\n",
    "        self.Uh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
    "        \n",
    "        # Biases for hidden vectors of shape (hidden_size,)\n",
    "        self.br = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='br')\n",
    "        self.bz = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bz')\n",
    "        self.bh = tf.Variable(tf.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
    "        \n",
    "        # Define the input layer placeholder\n",
    "        self.input_holder = tf.placeholder(dtype=dtype, shape=(None, time_steps, input_dimensions), name='input')\n",
    "        \n",
    "        self.input_holder = tf.reshape(tf.input_holder,(-1,time_steps,input_dimensions))\n",
    "        self.decay_affector1 = tf.reshape(tf.decay_affector,(-1,time_steps,input_dimensions))\n",
    "        \n",
    "        # Put the time-dimension upfront for the scan operator\n",
    "        self.x_t = tf.transpose(self.input_holder, [1, 0, 2], name='x_t')\n",
    "        self.decay_affector11 = tf.transpose(self.decay_affector1, [1, 0, 2], name='d_t')\n",
    "        \n",
    "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
    "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=dtype, shape=(input_dimensions, hidden_size)), name='h_0')\n",
    "        \n",
    "        # Perform the scan operator\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, (self.x_t, self.decay_affector11), initializer=self.h_0, name='h_t_transposed')\n",
    "        \n",
    "        # Transpose the result back\n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "        \n",
    "    def train(self,):\n",
    "        w_out = weight_variable([self.hidden_size*1,self.output_len*self.input_dimensions])\n",
    "        b_out = bias_variable([self.output_len*self.input_dimensions])\n",
    "\n",
    "        self.pred = tf.nn.elu(tf,matmul(self.h_t[:,-1,:],w_out)+b_out)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.squared_difference(pred,self.output_holder))\n",
    "        \n",
    "        self.trainOp = tf.train.AdadeltaOptimizer(learning_rate=0.002).minimize(loss)\n",
    "    \n",
    "    def forward_pass(self, h_tm1, c_t):\n",
    "        \"\"\"Perform a forward pass.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h_tm1: np.matrix\n",
    "            The hidden state at the previous timestep (h_{t-1}).\n",
    "        x_t: np.matrix\n",
    "            The input vector.\n",
    "        \"\"\"\n",
    "        (x_t, d_t) = c_t\n",
    "        # Definitions of update gate\n",
    "        z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
    "        # Definition of reset gate\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
    "        \n",
    "        # Definition of candidate actvation\n",
    "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
    "        \n",
    "        # feature decay\n",
    "        self.feature_decay = tf.exp(-tf.maximize(tf.matmul(d_t,self.wd2)+self.bd2,0))\n",
    "        \n",
    "        h_proposal_ = tf.multiply(self.feature_decay, h_proposal)\n",
    "        \n",
    "        # Compute the actual next hidden state\n",
    "        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal_)\n",
    "        \n",
    "        return h_t  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data,input_len=6,output_len=1, mr=0.2):\n",
    "    input_set=[]\n",
    "    mask_set=[]\n",
    "    output_set=[]\n",
    "    emprical_set=[] # mean set\n",
    "    base_set = []\n",
    "    decay_affector_set = []\n",
    "    \n",
    "    empriacal = np.mean(data,axis=1)\n",
    "    empriacal_full =np.array([empriacal,]*input_len)\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        origin_input = data[:,i:i+input_len]\n",
    "        condition = np.random.binomial(1, 1-missing_rate, np.shape(origin_input)[0]*np.shape(origin_input)[1])\n",
    "        condition = np.reshape(condition,(np.shape(origin_input)[0],np.shape(origin_input)[1]))\n",
    "        mask = np.where(condition<1,0.,1.)\n",
    "        origin_output = data[:,i+input_len:i+input_len+output_len]\n",
    "        \n",
    "        \n",
    "        \n",
    "        sparse_input = np.transpose(np.multiply(origin_input, mask)).reshape(-1,)\n",
    "        mask = np.transpose(mask).reshape(1,-1)\n",
    "        \n",
    "        origin_input_T = np.transpose(origin_input).reshape(-1,)\n",
    "        t = origin_input_T.shape[1]-1\n",
    "        \n",
    "        while t>=0:\n",
    "            base = []\n",
    "            decay_affector = []\n",
    "            for r in range(data.shape[0]):\n",
    "                tt = t-r\n",
    "                k = 0\n",
    "                while sparse_input[tt]<=0 and tt>=0:\n",
    "                    tt-=data.shape[0]\n",
    "                    k+=1\n",
    "                if origin_input_T[tt]<=0:\n",
    "                    base.append(-1)\n",
    "                else:\n",
    "                    base.append(origin_input_T[tt])\n",
    "                decay_affector.append(k)\n",
    "                \n",
    "            t-=data.shape[0]\n",
    "            \n",
    "        base = base[::-1]\n",
    "        decay_affector = decay_affector[::-1]\n",
    "        \n",
    "        input_set.append(sparse_input)\n",
    "        mask_set.append(mask)\n",
    "        output_set.append(np.transpose(origin_output))\n",
    "        emprical_set.append(empriacal_full.reshape(-1,))\n",
    "        base_set.append(base)\n",
    "        decay_affector_set.append(decay_affector)\n",
    "        \n",
    "    return input_set, output_set, mask_set, emprical_set, base_set, decay_affector_set\n",
    "\n",
    "def generate_batch(batch_size=32, epoch=0,input_set, output_set, mask_set, emprical_set, base_set, decay_affector_set):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    mask_batch = []\n",
    "    emprical_batch = []\n",
    "    base_batch = []\n",
    "    decay_affector_batch=[]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        t = epoch\n",
    "        input_batch.append(input_set[t])\n",
    "        output_batch.append(output_set[t])\n",
    "        mask_batch.append(mask_set[t])\n",
    "        emprical_batch.append(emprical_set[t])\n",
    "        base_batch.append(base_set[t])\n",
    "        decay_affector_batch.append(decay_affector_set[t])\n",
    "        \n",
    "        epoch+=1\n",
    "    return input_batch, output_batch, mask_batch, emprical_batch, base_batch, decay_affector_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model = GRU_D()\n",
    "model.train()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "max_episode = 10\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(max_episode):\n",
    "    for j in range():\n",
    "        _ = sess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n",
      "[[1 2]\n",
      " [7 8]]\n"
     ]
    }
   ],
   "source": [
    "def f(x, ys):\n",
    "  (y1, y2) = ys\n",
    "  return x + y1 * y2\n",
    "\n",
    "a = tf.constant([1, 2, 3, 4, 5,6,7,8,9,10,11,12])\n",
    "a1 = tf.reshape(a,(-1,6))\n",
    "a2 = tf.reshape(a1,(-1,3,2))\n",
    "a3 = tf.transpose(a2, [1, 0, 2], name='x_t')\n",
    "with tf.Session() as sess:\n",
    "      print(sess.run(a1))\n",
    "      print(sess.run(a3[0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40.893 , 41.938 ],\n",
       "       [50.319 , 51.13  ],\n",
       "       [53.88  , 53.607 ],\n",
       "       [37.305 , 37.501 ],\n",
       "       [38.388 , 38.383 ],\n",
       "       [49.13  , 47.303 ],\n",
       "       [35.466 , 36.961 ],\n",
       "       [57.445 , 59.567 ],\n",
       "       [39.697 , 41.162 ],\n",
       "       [48.646 , 48.309 ],\n",
       "       [50.983 , 50.916 ],\n",
       "       [42.27  , 43.874 ],\n",
       "       [45.276 , 46.113 ],\n",
       "       [48.706 , 49.31  ],\n",
       "       [10.621 , 20.377 ],\n",
       "       [38.202 , 41.944 ],\n",
       "       [50.492 , 52.671 ],\n",
       "       [35.824 , 35.758 ],\n",
       "       [49.225 , 49.352 ],\n",
       "       [49.294 , 49.224 ],\n",
       "       [39.855 , 37.938 ],\n",
       "       [44.002 , 42.115 ],\n",
       "       [38.834 , 42.076 ],\n",
       "       [41.276 , 39.868 ],\n",
       "       [37.452 , 36.12  ],\n",
       "       [46.714 , 47.224 ],\n",
       "       [42.157 , 42.03  ],\n",
       "       [45.234 , 45.608 ],\n",
       "       [45.205 , 41.881 ],\n",
       "       [37.552 , 39.472 ],\n",
       "       [40.196 , 37.175 ],\n",
       "       [42.26  , 45.563 ],\n",
       "       [38.455 , 38.246 ],\n",
       "       [37.342 , 39.057 ],\n",
       "       [42.787 , 41.327 ],\n",
       "       [45.891 , 45.874 ],\n",
       "       [54.963 , 56.746 ],\n",
       "       [39.511 , 39.56  ],\n",
       "       [50.104 , 48.273 ],\n",
       "       [54.054 , 54.1   ],\n",
       "       [41.171 , 40.705 ],\n",
       "       [38.03  , 37.46  ],\n",
       "       [44.656 , 40.522 ],\n",
       "       [46.199 , 45.97  ],\n",
       "       [39.454 , 37.011 ],\n",
       "       [43.059 , 41.715 ],\n",
       "       [50.5   , 50.593 ],\n",
       "       [ 0.    ,  0.    ],\n",
       "       [58.592 , 62.06  ],\n",
       "       [52.294 , 53.996 ],\n",
       "       [55.19  , 57.215 ],\n",
       "       [40.98  , 41.129 ],\n",
       "       [46.473 , 47.043 ],\n",
       "       [56.108 , 55.476 ],\n",
       "       [ 0.    ,  0.    ],\n",
       "       [54.37  , 53.84  ],\n",
       "       [55.528 , 54.848 ],\n",
       "       [54.815 , 54.197 ],\n",
       "       [21.31  , 21.011 ],\n",
       "       [47.025 , 43.978 ],\n",
       "       [39.932 , 39.357 ],\n",
       "       [56.976 , 57.174 ],\n",
       "       [40.971 , 39.277 ],\n",
       "       [52.685 , 52.346 ],\n",
       "       [51.172 , 50.566 ],\n",
       "       [51.921 , 52.04  ],\n",
       "       [42.832 , 44.695 ],\n",
       "       [46.194 , 46.158 ],\n",
       "       [39.936 , 41.012 ],\n",
       "       [53.528 , 53.382 ],\n",
       "       [36.016 , 35.612 ],\n",
       "       [38.066 , 37.898 ],\n",
       "       [32.647 , 36.521 ],\n",
       "       [46.696 , 48.455 ],\n",
       "       [41.767 , 41.624 ],\n",
       "       [34.754 , 37.258 ],\n",
       "       [44.523 , 45.941 ],\n",
       "       [54.133 , 53.084 ],\n",
       "       [51.062 , 50.678 ],\n",
       "       [54.523 , 49.598 ],\n",
       "       [43.042 , 44.421 ],\n",
       "       [41.782 , 43.267 ],\n",
       "       [35.69  , 37.256 ],\n",
       "       [46.103 , 44.794 ],\n",
       "       [33.71  , 34.525 ],\n",
       "       [39.825 , 38.874 ],\n",
       "       [39.749 , 40.151 ],\n",
       "       [38.759 , 39.675 ],\n",
       "       [54.301 , 54.5   ],\n",
       "       [33.997 , 35.378 ],\n",
       "       [37.445 , 38.399 ],\n",
       "       [41.561 , 41.277 ],\n",
       "       [52.997 , 53.254 ],\n",
       "       [51.035 , 52.373 ],\n",
       "       [34.963 , 34.998 ],\n",
       "       [46.772 , 47.171 ],\n",
       "       [48.505 , 49.23  ],\n",
       "       [51.604 , 50.859 ],\n",
       "       [44.552 , 45.111 ],\n",
       "       [42.321 , 43.633 ],\n",
       "       [36.326 , 38.819 ],\n",
       "       [37.915 , 39.386 ],\n",
       "       [43.006 , 44.05  ],\n",
       "       [50.608 , 52.114 ],\n",
       "       [40.302 , 35.731 ],\n",
       "       [39.952 , 39.201 ],\n",
       "       [42.954 , 43.447 ],\n",
       "       [56.402 , 57.065 ],\n",
       "       [48.321 , 47.601 ],\n",
       "       [41.941 , 42.238 ],\n",
       "       [54.89  , 51.307 ],\n",
       "       [51.914 , 50.435 ],\n",
       "       [57.13  , 59.992 ],\n",
       "       [32.807 , 32.995 ],\n",
       "       [48.21  , 48.771 ],\n",
       "       [52.342 , 52.347 ],\n",
       "       [39.533 , 41.876 ],\n",
       "       [51.713 , 52.07  ],\n",
       "       [50.962 , 53.533 ],\n",
       "       [36.947 , 36.171 ],\n",
       "       [44.643 , 42.632 ],\n",
       "       [54.96  , 54.694 ],\n",
       "       [ 0.    ,  0.    ],\n",
       "       [44.865 , 42.311 ],\n",
       "       [37.759 , 30.718 ],\n",
       "       [41.064 , 39.834 ],\n",
       "       [44.395 , 46.249 ],\n",
       "       [39.705 , 40.534 ],\n",
       "       [49.406 , 49.568 ],\n",
       "       [27.286 , 29.478 ],\n",
       "       [37.898 , 44.578 ],\n",
       "       [41.817 , 44.917 ],\n",
       "       [39.266 , 39.904 ],\n",
       "       [41.306 , 42.478 ],\n",
       "       [40.554 , 40.015 ],\n",
       "       [46.858 , 45.023 ],\n",
       "       [41.343 , 39.289 ],\n",
       "       [47.392 , 46.685 ],\n",
       "       [50.764 , 49.538 ],\n",
       "       [42.406 , 42.147 ],\n",
       "       [35.918 , 35.372 ],\n",
       "       [38.013 , 37.923 ],\n",
       "       [52.707 , 52.618 ],\n",
       "       [49.456 , 48.317 ],\n",
       "       [47.933 , 47.489 ],\n",
       "       [ 0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ],\n",
       "       [39.272 , 37.533 ],\n",
       "       [40.963 , 41.798 ],\n",
       "       [53.9   , 53.726 ],\n",
       "       [54.353 , 52.605 ],\n",
       "       [44.584 , 44.264 ],\n",
       "       [43.357 , 44.576 ],\n",
       "       [51.2   , 50.039 ],\n",
       "       [56.798 , 57.754 ],\n",
       "       [46.536 , 44.785 ],\n",
       "       [48.898 , 49.064 ],\n",
       "       [34.637 , 40.811 ],\n",
       "       [64.282 , 65.711 ],\n",
       "       [50.354 , 51.391 ],\n",
       "       [40.148 , 42.778 ],\n",
       "       [48.007 , 49.149 ],\n",
       "       [41.215 , 41.667 ],\n",
       "       [51.371 , 52.152 ],\n",
       "       [53.533 , 55.407 ],\n",
       "       [50.729 , 47.677 ],\n",
       "       [38.935 , 38.92  ],\n",
       "       [41.375 , 36.395 ],\n",
       "       [35.616 , 35.775 ],\n",
       "       [44.547 , 43.246 ],\n",
       "       [35.082 , 37.196 ],\n",
       "       [46.344 , 46.643 ],\n",
       "       [40.59  , 45.402 ],\n",
       "       [37.439 , 38.535 ],\n",
       "       [47.089 , 51.311 ],\n",
       "       [39.668 , 37.06  ],\n",
       "       [39.116 , 41.763 ],\n",
       "       [36.049 , 38.526 ],\n",
       "       [38.907 , 40.492 ],\n",
       "       [38.583 , 39.471 ],\n",
       "       [47.797 , 49.173 ],\n",
       "       [50.101 , 48.751 ],\n",
       "       [50.236 , 50.808 ],\n",
       "       [57.149 , 56.176 ],\n",
       "       [52.675 , 53.665 ],\n",
       "       [38.027 , 41.162 ],\n",
       "       [47.053 , 50.066 ],\n",
       "       [38.467 , 38.474 ],\n",
       "       [42.724 , 42.923 ],\n",
       "       [53.773 , 55.427 ],\n",
       "       [42.11  , 42.804 ],\n",
       "       [40.455 , 45.138 ],\n",
       "       [37.661 , 41.731 ],\n",
       "       [42.228 , 43.095 ],\n",
       "       [35.524 , 39.655 ],\n",
       "       [43.847 , 45.478 ],\n",
       "       [49.827 , 49.686 ],\n",
       "       [50.58  , 48.539 ],\n",
       "       [35.48  , 35.814 ],\n",
       "       [57.05  , 57.141 ],\n",
       "       [60.681 , 60.128 ],\n",
       "       [44.956 , 44.181 ],\n",
       "       [38.764 , 41.128 ],\n",
       "       [49.34  , 49.209 ],\n",
       "       [54.202 , 52.464 ],\n",
       "       [49.972 , 51.565 ],\n",
       "       [39.584 , 40.914 ],\n",
       "       [35.971 , 34.766 ],\n",
       "       [57.129 , 58.145 ],\n",
       "       [65.137 , 66.56  ],\n",
       "       [61.977 , 62.0745],\n",
       "       [39.396 , 40.771 ],\n",
       "       [36.245 , 36.987 ],\n",
       "       [51.88  , 50.378 ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:,:2]\n",
    "# np.transpose(tensor[:,:2]).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.mean(tensor[:,:2],axis=0)\n",
    "b=np.array([a,]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43.80407009, 44.14160047],\n",
       "       [43.80407009, 44.14160047],\n",
       "       [43.80407009, 44.14160047]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43.80407009, 44.14160047, 43.80407009, 44.14160047, 43.80407009,\n",
       "       44.14160047])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
